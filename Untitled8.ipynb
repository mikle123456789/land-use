{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели для выделения облаков на снимках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "from tensorflow import keras\n",
    "import segmentation_models as sm\n",
    "import segmentation_models as sm\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "print('*'*30)\n",
    "print('Loading and preprocessing train data...')\n",
    "print('*'*30)\n",
    "\n",
    "def train1():\n",
    "        path = '38-Cloud_training'\n",
    "        list = os.listdir(path + '/images/train')\n",
    "        listimg = list\n",
    "        l1 = np.random.permutation(listimg)\n",
    "        for j in range(len(l1)):\n",
    "                i = l1[j]\n",
    "                img = Image.open(os.path.join(path, 'images', 'train', i))\n",
    "                img = img.resize((256, 256))\n",
    "                img_train = np.asarray(img).astype('float16') / 255\n",
    "                if os.path.exists(os.path.join(path, 'labels', i[:-3] + 'png')):\n",
    "                    img = np.array(Image.open(os.path.join(path, 'labels', i[:-3] + 'png')).resize((256, 256)))\n",
    "                else:\n",
    "                    img = np.array(Image.open(os.path.join(path, 'labels', i[:-3] + 'TIF')).resize((256, 256)))\n",
    "                mask_train = np.asarray(img).astype('float16').reshape((256, 256, 1)) / 255\n",
    "                yield tf.convert_to_tensor(img_train), tf.convert_to_tensor(mask_train)\n",
    "\n",
    "\n",
    "def valid1():\n",
    "        path = '38-Cloud_training'\n",
    "        list = os.listdir(path + '/images/valid')\n",
    "        listimg = list\n",
    "        l1 = np.random.permutation(listimg)\n",
    "        for j in range(len(l1)):\n",
    "                i = l1[j]\n",
    "                img = Image.open(os.path.join(path,'images', 'valid', i))\n",
    "                img = img.resize((256, 256))\n",
    "                img_train = np.asarray(img).astype('float16') / 255\n",
    "\n",
    "                if os.path.exists(os.path.join(path, 'labels', i[:-3] + 'png')):\n",
    "                    img = np.array(Image.open(os.path.join(path, 'labels', i[:-3] + 'png')).resize((256, 256)))\n",
    "                else:\n",
    "                    img = np.array(Image.open(os.path.join(path, 'labels', i[:-3] + 'TIF')).resize((256, 256)))\n",
    "                mask_train = np.asarray(img).astype('float16').reshape((256, 256, 1)) / 255\n",
    "                yield tf.convert_to_tensor(img_train), tf.convert_to_tensor(mask_train)\n",
    "\n",
    "def test1:\n",
    "        path = '38-Cloud_training'\n",
    "        list = os.listdir(path + '/images/test')\n",
    "        listimg = list\n",
    "        l1 = np.random.permutation(listimg)\n",
    "        for j in range(len(l1)):\n",
    "                i = l1[j]\n",
    "                img = Image.open(os.path.join(path,'images', 'test', i))\n",
    "                img = img.resize((256, 256))\n",
    "                img_train = np.asarray(img).astype('float16') / 255\n",
    "\n",
    "                if os.path.exists(os.path.join(path, 'labels', i[:-3] + 'png')):\n",
    "                    img = np.array(Image.open(os.path.join(path, 'labels', i[:-3] + 'png')).resize((256, 256)))\n",
    "                else:\n",
    "                    img = np.array(Image.open(os.path.join(path, 'labels', i[:-3] + 'TIF')).resize((256, 256)))\n",
    "                mask_train = np.asarray(img).astype('float16').reshape((256, 256, 1)) / 255\n",
    "                yield tf.convert_to_tensor(img_train), tf.convert_to_tensor(mask_train)\n",
    "\n",
    "print('*'*30)\n",
    "print('Creating and compiling model...')\n",
    "print('*'*30)\n",
    "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "#Выбор модели\n",
    "models = {'Linknet':sm.Linknet, 'FPN':sm.FPN, 'Unet': sm.Unet}\n",
    "val_f1 = {'Unet': 0, 'FPN' : 0, 'Linknet' : 0}\n",
    "\n",
    "for model_str in models:\n",
    "    #print(model_str)\n",
    "    size = 512\n",
    "    with strategy.scope():\n",
    "        model = models[model_str](classes = 1, activation = 'sigmoid')\n",
    "        model.compile(optimizer = 'adam', loss = sm.losses.dice_loss, metrics = ['accuracy', sm.metrics.f1_score])\n",
    "    print('*'*30)\n",
    "    print('Fitting model ' + model_str + '...')\n",
    "    print('*'*30)\n",
    "    s = 'images'\n",
    "    size = 256\n",
    "    train = tf.data.Dataset.from_generator(train1, output_signature=(\n",
    "             tf.TensorSpec(shape=(size, size, 3), dtype=tf.float16),\n",
    "             tf.TensorSpec(shape=(size, size, 1), dtype=tf.float16))).batch(12)\n",
    "\n",
    "    valid = tf.data.Dataset.from_generator(valid1, output_signature=(\n",
    "             tf.TensorSpec(shape=(size, size, 3), dtype=tf.float16),\n",
    "             tf.TensorSpec(shape=(size, size, 1), dtype=tf.float16))).batch(12)\n",
    "\n",
    "    epochs = 3\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10)\n",
    "    model_checkpoint = ModelCheckpoint(f'weights_' + model_str + f'_buildings_{epochs}epochs.tf', monitor='val_accuracy', save_best_only=True, mode = 'max')\n",
    "\n",
    "    history =  model.fit(train, epochs=epochs, verbose=1,\n",
    "              validation_data = valid,\n",
    "              callbacks=[model_checkpoint, early_stopping])\n",
    "\n",
    "    val_f1[model_str] = np.max(np.asarray(history.history['val_accuracy']))\n",
    "    print(val_f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 'Unet'\n",
    "print(val_f1)\n",
    "for i in models:\n",
    "    if val_f1[i] > val_f1[res]:\n",
    "        res = i\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "model_str = res\n",
    "#'Выбор функции потерь\n",
    "losses = {'jaccard_loss' : sm.losses.jaccard_loss, 'dice_loss' : sm.losses.dice_loss, 'focal_loss' : sm.losses.categorical_focal_loss, 'crossentropy' : sm.losses.categorical_crossentropy, 'cce_dice_loss' : sm.losses.cce_dice_loss, 'bce_jaccard_loss' : sm.losses.cce_jaccard_loss, 'focal_dice_loss' : sm.losses.categorical_focal_dice_loss, 'focal_jaccard_loss' : sm.losses.categorical_focal_jaccard_loss}\n",
    "val_f1 = {'jaccard_loss':0, 'dice_loss':0, 'focal_loss':0, 'crossentropy':0, 'cce_dice_loss':0, 'bce_jaccard_loss':0, 'focal_dice_loss':0, 'focal_jaccard_loss':0}\n",
    "def f1_loss (y_true, y_pred):\n",
    "\n",
    "    tp = y_true*y_pred\n",
    "    tn = (1-y_true)*(1-y_pred)\n",
    "    fp = (1-y_true)*y_pred\n",
    "    fn = y_true*(1-y_pred)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    return 1 - f1\n",
    "for loss in losses:\n",
    "    #print(model_str)\n",
    "    size = 256\n",
    "    with strategy.scope():\n",
    "        model = models[res](classes = 1, activation = 'sigmoid')\n",
    "        model.compile(optimizer = 'adam', loss = losses[loss], metrics = ['accuracy', sm.metrics.f1_score])\n",
    "    print('*'*30)\n",
    "    print('Fitting model ' + res + f' with {loss}...')\n",
    "    print('*'*30)\n",
    "\n",
    "    train = tf.data.Dataset.from_generator(train1, output_signature=(\n",
    "             tf.TensorSpec(shape=(size, size, 3), dtype=tf.float16),\n",
    "             tf.TensorSpec(shape=(size, size, 1), dtype=tf.float16))).batch(12)\n",
    "\n",
    "    valid = tf.data.Dataset.from_generator(valid1, output_signature=(\n",
    "             tf.TensorSpec(shape=(size, size, 3), dtype=tf.float16),\n",
    "             tf.TensorSpec(shape=(size, size, 1), dtype=tf.float16))).batch(12)\n",
    "\n",
    "    epochs = 3\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_f1-score', patience=10)\n",
    "    model_checkpoint = ModelCheckpoint(f'weights_' + res + f'_buildings_{epochs}epochs.tf', monitor='val_f1-score', save_best_only=True, mode = 'max')\n",
    "\n",
    "    history =  model.fit(train, epochs=epochs, verbose=1,\n",
    "              validation_data = valid,\n",
    "              callbacks=[model_checkpoint, early_stopping])\n",
    "    val_f1[loss] = np.max(np.asarray(history.history['val_accuracy']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_l = 'focal_dice_loss'\n",
    "print(val_f1)\n",
    "for key in val_f1:\n",
    "    if val_f1[res_l] < val_f1[key]:\n",
    "        res_l = key\n",
    "losses = {'jaccard_loss' : sm.losses.jaccard_loss, 'dice_loss' : sm.losses.dice_loss, 'focal_loss' : sm.losses.categorical_focal_loss, 'crossentropy' : sm.losses.categorical_crossentropy, 'cce_dice_loss' : sm.losses.cce_dice_loss, 'bce_jaccard_loss' : sm.losses.cce_jaccard_loss, 'focal_dice_loss' : sm.losses.categorical_focal_dice_loss, 'focal_jaccard_loss' : sm.losses.categorical_focal_jaccard_loss}\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    size = 256\n",
    "    with strategy.scope():\n",
    "        model = models[res](classes = 1, activation = 'sigmoid')\n",
    "        model.compile(optimizer = 'adam', loss = losses[res_l], metrics = ['accuracy', sm.metrics.f1_score])\n",
    "    print('*'*30)\n",
    "    print('Fitting model ' + model_str + f' with {s}...')\n",
    "    print('*'*30)\n",
    "\n",
    "    train = tf.data.Dataset.from_generator(train1, output_signature=(\n",
    "             tf.TensorSpec(shape=(size, size, 3), dtype=tf.float16),\n",
    "             tf.TensorSpec(shape=(size, size, 1), dtype=tf.float16))).batch(12)\n",
    "\n",
    "    valid = tf.data.Dataset.from_generator(valid1, output_signature=(\n",
    "             tf.TensorSpec(shape=(size, size, 3), dtype=tf.float16),\n",
    "             tf.TensorSpec(shape=(size, size, 1), dtype=tf.float16))).batch(12)\n",
    "\n",
    "    epochs = 400\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=100)\n",
    "    model_checkpoint = ModelCheckpoint(f'res_cloud_weights_' + model_str + f'_buildings_{epochs}epochs.tf', monitor='val_accuracy', save_best_only=True, mode = 'max')\n",
    "\n",
    "    history =  model.fit(train, epochs=epochs, verbose=1,\n",
    "              validation_data = valid,\n",
    "              callbacks=[model_checkpoint, early_stopping])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
