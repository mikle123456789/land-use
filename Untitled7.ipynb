{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели по классификации типов землепользования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "from tensorflow import keras\n",
    "import segmentation_models as sm\n",
    "import segmentation_models as sm\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "print('*'*30)\n",
    "print('Loading and preprocessing train data...')\n",
    "print('*'*30)\n",
    "\n",
    "def train():\n",
    "        path = 'images/divided'\n",
    "        list = os.listdir(path + '/train')\n",
    "        listimg = list\n",
    "        l1 = np.random.permutation(listimg)\n",
    "        for j in range(len(l1)):\n",
    "                i = l1[j]\n",
    "                img = Image.open(os.path.join(path, 'train', i))\n",
    "                img = img.resize((256, 256))\n",
    "                img_train = np.asarray(img).astype('float16') / 255\n",
    "\n",
    "                img = Image.open(os.path.join('images/PZZ/old', i)).resize((256, 256))\n",
    "                mask_train = np.asarray(img)\n",
    "                mask_train1 = mask_train\n",
    "                if np.count_nonzero(mask_train1==0) < 256*256:\n",
    "                    m1 = np.zeros((256, 256, 8))\n",
    "                    for k in range(0, 8):\n",
    "                                if np.argwhere(mask_train1 == k).shape[0] != 0:\n",
    "                                    x = np.argwhere(mask_train1 == k)\n",
    "                                    m1[x[:, 0], x[:, 1] , k] = 1\n",
    "                    yield tf.convert_to_tensor(img_train), tf.convert_to_tensor(m1)\n",
    "\n",
    "\n",
    "def valid()):\n",
    "        path = 'images/divided'\n",
    "        list = os.listdir(path + '/valid')\n",
    "        listimg = list\n",
    "        l1 = np.random.permutation(listimg)\n",
    "        for j in range(len(l1)):\n",
    "                i = l1[j]\n",
    "                img = Image.open(os.path.join(path, 'valid', i))\n",
    "                img = img.resize((256, 256))\n",
    "                img_train = np.asarray(img).astype('float16') / 255\n",
    "\n",
    "                img = Image.open(os.path.join('images/PZZ/old', i)).resize((256, 256))\n",
    "                mask_train = np.asarray(img)\n",
    "                mask_train1 = mask_train\n",
    "                if np.count_nonzero(mask_train1==0) < 256*256:\n",
    "                    m1 = np.zeros((256, 256, 8))\n",
    "                    for k in range(0, 8):\n",
    "                                if np.argwhere(mask_train1 == k).shape[0] != 0:\n",
    "                                    x = np.argwhere(mask_train1 == k)\n",
    "                                    m1[x[:, 0], x[:, 1] , k] = 1\n",
    "                    yield tf.convert_to_tensor(img_train), tf.convert_to_tensor(m1)\n",
    "\n",
    "def test_data(data):\n",
    "        path = 'images/divided'\n",
    "        list = os.listdir(path + '/test')\n",
    "        listimg = list\n",
    "        l1 = np.random.permutation(listimg)\n",
    "        for j in range(len(l1)):\n",
    "                i = l1[j]\n",
    "                img = Image.open(os.path.join(path, 'test', i))\n",
    "                img = img.resize((256, 256))\n",
    "                img_train = np.asarray(img).astype('float16') / 255\n",
    "\n",
    "                img = Image.open(os.path.join('images/PZZ/old', i)).resize((256, 256))\n",
    "                mask_train = np.asarray(img)\n",
    "                mask_train1 = mask_train\n",
    "                if np.count_nonzero(mask_train1==0) < 256*256:\n",
    "                    m1 = np.zeros((256, 256, 8))\n",
    "                    for k in range(0, 8):\n",
    "                                if np.argwhere(mask_train1 == k).shape[0] != 0:\n",
    "                                    x = np.argwhere(mask_train1 == k)\n",
    "                                    m1[x[:, 0], x[:, 1] , k] = 1\n",
    "                    yield tf.convert_to_tensor(img_train), tf.convert_to_tensor(m1)\n",
    "print('*'*30)\n",
    "print('Creating and compiling model...')\n",
    "print('*'*30)\n",
    "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "#'PSPNet' : sm.PSPNet,\n",
    "models = {'Linknet':sm.Linknet, 'FPN':sm.FPN, 'Unet': sm.Unet}\n",
    "val_f1 = {'Unet': 0, 'FPN' : 0, 'Linknet' : 0}\n",
    "\n",
    "for model_str in models:\n",
    "    #print(model_str)\n",
    "    size = 512\n",
    "    with strategy.scope():\n",
    "        model = models[model_str](classes = 8, activation = 'sigmoid')\n",
    "        model.compile(optimizer = 'adam', loss = sm.losses.dice_loss, metrics = ['accuracy', sm.metrics.f1_score])\n",
    "    print('*'*30)\n",
    "    print('Fitting model ' + model_str + '...')\n",
    "    print('*'*30)\n",
    "    s = 'images'\n",
    "    size = 256\n",
    "    train = tf.data.Dataset.from_generator(train, output_signature=(\n",
    "             tf.TensorSpec(shape=(size, size, 3), dtype=tf.float16),\n",
    "             tf.TensorSpec(shape=(size, size, 8), dtype=tf.float16))).batch(12)\n",
    "\n",
    "    valid = tf.data.Dataset.from_generator(valid, output_signature=(\n",
    "             tf.TensorSpec(shape=(size, size, 3), dtype=tf.float16),\n",
    "             tf.TensorSpec(shape=(size, size, 8), dtype=tf.float16))).batch(12)\n",
    "\n",
    "    epochs = 3\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10)\n",
    "    model_checkpoint = ModelCheckpoint(f'weights_' + model_str + f'_buildings_{epochs}epochs.tf', monitor='val_accuracy', save_best_only=True, mode = 'max')\n",
    "\n",
    "    history =  model.fit(train, epochs=epochs, verbose=1,\n",
    "              validation_data = valid,\n",
    "              callbacks=[model_checkpoint, early_stopping])\n",
    "\n",
    "    val_f1[model_str] = np.max(np.asarray(history.history['val_accuracy']))\n",
    "    print(val_f1)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 'Unet'\n",
    "print(val_f1)\n",
    "for i in models:\n",
    "    if val_f1[i] > val_f1[res]:\n",
    "        res = i\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "model_str = res\n",
    "#'PSPNet' : sm.PSPNet,\n",
    "losses = {'jaccard_loss' : sm.losses.jaccard_loss, 'dice_loss' : sm.losses.dice_loss, 'focal_loss' : sm.losses.categorical_focal_loss, 'crossentropy' : sm.losses.categorical_crossentropy, 'cce_dice_loss' : sm.losses.cce_dice_loss, 'bce_jaccard_loss' : sm.losses.cce_jaccard_loss, 'focal_dice_loss' : sm.losses.categorical_focal_dice_loss, 'focal_jaccard_loss' : sm.losses.categorical_focal_jaccard_loss}\n",
    "val_f1 = {'jaccard_loss':0, 'dice_loss':0, 'focal_loss':0, 'crossentropy':0, 'cce_dice_loss':0, 'bce_jaccard_loss':0, 'focal_dice_loss':0, 'focal_jaccard_loss':0}\n",
    "def f1_loss (y_true, y_pred):\n",
    "\n",
    "    tp = y_true*y_pred\n",
    "    tn = (1-y_true)*(1-y_pred)\n",
    "    fp = (1-y_true)*y_pred\n",
    "    fn = y_true*(1-y_pred)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    return 1 - f1\n",
    "for loss in losses:\n",
    "    #print(model_str)\n",
    "    size = 256\n",
    "    with strategy.scope():\n",
    "        model = models[res](classes = 8, activation = 'sigmoid')\n",
    "        model.compile(optimizer = 'adam', loss = losses[loss], metrics = ['accuracy', sm.metrics.f1_score])\n",
    "    print('*'*30)\n",
    "    print('Fitting model ' + res + f' with {loss}...')\n",
    "    print('*'*30)\n",
    "\n",
    "    train = tf.data.Dataset.from_generator(train1, output_signature=(\n",
    "             tf.TensorSpec(shape=(size, size, 3), dtype=tf.float16),\n",
    "             tf.TensorSpec(shape=(size, size, 8), dtype=tf.float16))).batch(12)\n",
    "\n",
    "    valid = tf.data.Dataset.from_generator(valid1, output_signature=(\n",
    "             tf.TensorSpec(shape=(size, size, 3), dtype=tf.float16),\n",
    "             tf.TensorSpec(shape=(size, size, 8), dtype=tf.float16))).batch(12)\n",
    "\n",
    "    epochs = 3\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_f1-score', patience=10)\n",
    "    model_checkpoint = ModelCheckpoint(f'weights_' + res + f'_buildings_{epochs}epochs.tf', monitor='val_f1-score', save_best_only=True, mode = 'max')\n",
    "\n",
    "    history =  model.fit(train, epochs=epochs, verbose=1,\n",
    "              validation_data = valid,\n",
    "              callbacks=[model_checkpoint, early_stopping])\n",
    "    val_f1[loss] = np.max(np.asarray(history.history['val_accuracy']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_l = 'focal_dice_loss'\n",
    "print(val_f1)\n",
    "for key in val_f1:\n",
    "    if val_f1[res_l] < val_f1[key]:\n",
    "        res_l = key\n",
    "losses = {'jaccard_loss' : sm.losses.jaccard_loss, 'dice_loss' : sm.losses.dice_loss, 'focal_loss' : sm.losses.categorical_focal_loss, 'crossentropy' : sm.losses.categorical_crossentropy, 'cce_dice_loss' : sm.losses.cce_dice_loss, 'bce_jaccard_loss' : sm.losses.cce_jaccard_loss, 'focal_dice_loss' : sm.losses.categorical_focal_dice_loss, 'focal_jaccard_loss' : sm.losses.categorical_focal_jaccard_loss}\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #print(model_str)\n",
    "    size = 256\n",
    "    with strategy.scope():\n",
    "        model = models[res](classes = 8, activation = 'sigmoid')\n",
    "        model.compile(optimizer = 'adam', loss = losses[res_l], metrics = ['accuracy', sm.metrics.f1_score])\n",
    "    print('*'*30)\n",
    "    print('Fitting model ' + model_str + f' with {s}...')\n",
    "    print('*'*30)\n",
    "\n",
    "    train = tf.data.Dataset.from_generator(train1, output_signature=(\n",
    "             tf.TensorSpec(shape=(size, size, 3), dtype=tf.float16),\n",
    "             tf.TensorSpec(shape=(size, size, 8), dtype=tf.float16))).batch(12)\n",
    "\n",
    "    valid = tf.data.Dataset.from_generator(valid1, output_signature=(\n",
    "             tf.TensorSpec(shape=(size, size, 3), dtype=tf.float16),\n",
    "             tf.TensorSpec(shape=(size, size, 8), dtype=tf.float16))).batch(12)\n",
    "\n",
    "    epochs = 400\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=100)\n",
    "    model_checkpoint = ModelCheckpoint(f'res_weights_' + model_str + f'_buildings_{epochs}epochs.tf', monitor='val_accuracy', save_best_only=True, mode = 'max')\n",
    "\n",
    "    history =  model.fit(train, epochs=epochs, verbose=1,\n",
    "              validation_data = valid,\n",
    "              callbacks=[model_checkpoint, early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'a' is an invalid keyword argument for print()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ece0a1ca46d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'a' is an invalid keyword argument for print()"
     ]
    }
   ],
   "source": [
    "a = 10\n",
    "b = 5\n",
    "print(a=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
